import os

import numpy as np
import pandas as pd
from openai import OpenAI
from scipy.stats import chi2_contingency, entropy
from sklearn.metrics import confusion_matrix

from agents import BaseAgent, ClassificationAgent, build_messages, load_categories_from_yaml
from gmail_api import Mail


class MetricCalculator:
    """Consistency ë° Correctness í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤."""

    @staticmethod
    def compute_metrics(results: list, ground_truth: list):
        """
        ì£¼ì–´ì§„ ê²°ê³¼ì— ëŒ€í•´ Consistency ë° Correctness í‰ê°€ ìˆ˜í–‰.

        Args:
            results (list): ë¶„ë¥˜ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸.
            ground_truth (list): ì‹¤ì œ ì •ë‹µ ë°ì´í„° ë¦¬ìŠ¤íŠ¸.

        Returns:
            tuple: (entropy_value, diversity_index, p_value, accuracy)
        """
        value_counts = pd.Series(results).value_counts(normalize=True)
        entropy_value = entropy(value_counts)
        diversity_index = len(value_counts) / len(results)
        chi2, p_value, _, _ = chi2_contingency(pd.crosstab(pd.Series(results), pd.Series(results)))
        conf_matrix = confusion_matrix(ground_truth, results, labels=np.unique(ground_truth + results))
        accuracy = np.trace(conf_matrix) / np.sum(conf_matrix)

        return entropy_value, diversity_index, p_value, accuracy


class EvaluationDataFrameManager:
    """
    í‰ê°€ ë°ì´í„° í”„ë ˆì„ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤.
    """

    def __init__(self, inference_count: int):
        self.output_dir = "evaluation/classification/label_data"
        os.makedirs(self.output_dir, exist_ok=True)
        self.csv_file_path = os.path.join(self.output_dir, "labeled.csv")

        self.columns = (
            ["mail_id"]
            + [f"inference_{i+1}" for i in range(inference_count)]
            + ["entropy", "diversity_index", "chi_square_p_value", "accuracy"]
        )

        if os.path.exists(self.csv_file_path):
            self.eval_df = pd.read_csv(self.csv_file_path)
            print(f"ğŸ“„ ê¸°ì¡´ í‰ê°€ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {self.eval_df.shape[0]}ê°œì˜ ë°ì´í„°")
        else:
            self.eval_df = pd.DataFrame(columns=self.columns)

    def update_eval_df(self, mail_id: str, results: list, ground_truth: list):
        if mail_id in self.eval_df["mail_id"].values:
            print(f"âš ï¸ ì´ë¯¸ ì²˜ë¦¬ëœ ë©”ì¼ (ID: {mail_id}), ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        entropy_value, diversity_index, p_value, accuracy = MetricCalculator.compute_metrics(results, ground_truth)
        new_entry = pd.DataFrame(
            [[mail_id] + results + [entropy_value, diversity_index, p_value, accuracy]], columns=self.columns
        )
        self.eval_df = pd.concat([self.eval_df, new_entry], ignore_index=True)
        self.eval_df.to_csv(self.csv_file_path, index=False)

    def print_df(self):
        print(self.eval_df)


class ClassificationEvaluationAgent(BaseAgent):
    """
    GPT ê¸°ë°˜ ë¶„ë¥˜ í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ë©° Consistency ë° Correctnessë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” í´ë˜ìŠ¤.
    """

    def __init__(self, model: str, human_evaluation: bool, inference: int, temperature: int = None, seed: int = None):
        super().__init__(model, temperature, seed)
        self.inference_iteration = inference
        self.human_evaluation = human_evaluation
        self.df_manager = EvaluationDataFrameManager(inference)

    def initialize_chat(self, model, temperature=None, seed=None):
        return OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    def generate_ground_truth(self, mail: Mail) -> str:
        categories = load_categories_from_yaml(is_prompt=True)
        categories_text = "\n".join([f"ì¹´í…Œê³ ë¦¬ ëª…: {c['name']}\në¶„ë¥˜ ê¸°ì¤€: {c['rubric']}" for c in categories])

        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=build_messages(
                template_type="classification",
                target_range="single",
                action="classification",
                mail=str(mail),
                categories=categories_text,
            ),
        )
        return response.choices[0].message.content.strip()

    def process(self, mail: Mail, classifier: ClassificationAgent) -> Mail:
        ground_truth = self.generate_ground_truth(mail)

        if self.human_evaluation:
            user_input = input(
                f"Subject: {mail.subject}\në³´ë‚¸ ì‚¬ëŒ: {mail.sender}\në³¸ë¬¸: {mail.body}\n"
                f"===================\nì˜ˆì¸¡ëœ ì •ë‹µ: {ground_truth}. ìˆ˜ì •í•˜ë ¤ë©´ ì…ë ¥, ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë ¤ë©´ Enter: "
            )
            ground_truth = user_input.strip() if user_input else ground_truth

        results = [classifier.process(mail) for _ in range(self.inference_iteration)]
        ground_truth_list = [ground_truth] * len(results)
        self.df_manager.update_eval_df(mail.id, results, ground_truth_list)
        return mail

    def print_evaluation(self):
        self.df_manager.print_df()

    @staticmethod
    def calculate_token_cost():
        pass
