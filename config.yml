gmail:
  start_date: # gmail에서 불러올 시작 날짜 (값이 없는 경우 2025/01/10)
  end_date: # gmail에서 불러올 끝 날짜 (값이 없는 경우 오늘 날짜)
  max_mails: 15 # gmail에서 불러올 메일 최대 개수

# 전체 모델에 적용하는 seed와 temperature
seed: 42
temperature:
  summary: 0
  classification: 0

# 개별 메일 요약
self_refine:
  max_iteration: 3

# 최종 리포트 요약
reflexion:
  max_iteration: 3
  threshold_type: "average" # "average" | "all"
  threshold: 4.5

# G-Eval 프롬프트 경로
common_prompts: &common_prompts
  consistency: "prompt/template/g_eval/con_{eval_type}.txt"
  coherence: "prompt/template/g_eval/coh_{eval_type}.txt"
  fluency: "prompt/template/g_eval/flu_{eval_type}.txt"
  relevance: "prompt/template/g_eval/rel_{eval_type}.txt"
  readability: "prompt/template/g_eval/rdb_{eval_type}.txt"
  clearance: "prompt/template/g_eval/clr_{eval_type}.txt"
  practicality: "prompt/template/g_eval/prc_{eval_type}.txt"

token_tracking: true

# 평가 세팅
# Summary 평가 관련 설정
summary:
  metrics:
    - rouge
    - bert
    - g-eval

  bert_model: "distilbert-base-uncased"

  g_eval:
    openai_model: "gpt-4"
    additional: False # "readability", "clearance", "practicality"를 G-Eval에 적용할 여부
    prompts:
      <<: *common_prompts

# Report 평가 관련 설정
report:
  metrics:
    - g-eval

  g_eval:
    openai_model: "gpt-4o"
    additional: False # "readability", "clearance", "practicality"를 G-Eval에 적용할 여부
    prompts:
      <<: *common_prompts

classification:
  do_manual_filter: False
  inference: 1 # Consistency 평가 용 반복 추론 횟수 설정

embedding:
  model_name: "bge-m3" # "bge-m3" | "upstage"
  similarity_metric: "cosine-similarity" # "cosine-similarity" | "dot-product"
  similarity_threshold: 0.8
  save_results: true
