{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result í‰ê°€ ìŠ¤í¬ë¦½íŠ¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATED_SUMMARY_CSV = \"evaluation/data/generated_summary.csv\"\n",
    "GENERATED_CATEGORY_CSV = \"evaluation/data/generated_category.csv\"\n",
    "GENERATED_REPORT_CSV = \"evaluation/data/generated_report.csv\"\n",
    "\n",
    "REFERNECE_CSV = \"evaluation/data/reference.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import ë° í™˜ê²½ ì£¼ì…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from agents.classification.classification_type import ClassificationType\n",
    "from evaluation.classification.dataframe_manager import DataFrameManager\n",
    "from evaluation.evaluation_summary import evaluate_summary\n",
    "from evaluation.gpt_eval import calculate_g_eval\n",
    "from evaluation.result_printer import print_evaluation_results\n",
    "from utils.configuration import Config\n",
    "\n",
    "load_dotenv()\n",
    "Config.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(reference_df): 48\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>subject</th>\n",
       "      <th>category</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194e90265a3c53fe</td>\n",
       "      <td>*ì§€ì†ê°€ëŠ¥ì›ì´ ì¶”ì²œí•˜ëŠ” 2025-1 ì§€ì†ê°€ëŠ¥ êµê³¼ëª© 25ì„ !*\\n\\nESG? ì§€ì†ê°€...</td>\n",
       "      <td>[ì§€ì†ê°€ëŠ¥ì›] ì§€ì†ê°€ëŠ¥ì›ì´ ì¶”ì²œí•˜ëŠ” 2025-1 ì§€ì†ê°€ëŠ¥ êµê³¼ëª© 25ì„ !</td>\n",
       "      <td>other</td>\n",
       "      <td>read only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>194e343333e5ff27</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>ICTëª…í’ˆì¸ì¬ì–‘ì„±ì‚¬ì—…ë‹¨ ë‰´ìŠ¤ë ˆí„° ê²¨ìš¸í˜¸ë¥¼ ë³´ë‚´ ë“œë¦½ë‹ˆë‹¤.</td>\n",
       "      <td>academic</td>\n",
       "      <td>read only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>194e34039181a3c0</td>\n",
       "      <td>ì•ˆë…•í•˜ì„¸ìš”,\\n\\n\\n2024ë…„ ì—…ì í‰ê°€ë¥¼ ìœ„í•´ ì‹¤ì  ì—…ë°ì´íŠ¸ ìš”ì²­ì„ ë“œë¦½ë‹ˆë‹¤. ì´...</td>\n",
       "      <td>2024ë…„ ì—…ì í‰ê°€ë¥¼ ìœ„í•œ ì—°êµ¬ì‹¤ì  ì—…ë°ì´íŠ¸ ìš”ì²­ì˜ ê±´ (~2ì›” 7ì¼)</td>\n",
       "      <td>academic</td>\n",
       "      <td>action needed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194e334940f6bad0</td>\n",
       "      <td>ì•ˆë…•í•˜ì„¸ìš”\\nì¹œí™˜ê²½ ë””ì§€í„¸ ì •ë³´ê³¼í•™ êµìœ¡ì—°êµ¬ë‹¨ í•¨ì´ì—´ì…ë‹ˆë‹¤.\\n\\nBK21 ëŒ€í•™ì›í˜...</td>\n",
       "      <td>[BK] 2025ë…„ 2ì›” 7ì¼(ê¸ˆ) BK21 ëŒ€í•™ì›í˜ì‹ ì‚¬ì—… ì˜ì–´ë…¼ë¬¸ì‘ì„±ë²• ì›Œí¬ìˆ ì•ˆ...</td>\n",
       "      <td>academic</td>\n",
       "      <td>action needed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>194e325d62686180</td>\n",
       "      <td>Hi there,\\n\\nWe charged $5.20 to your credit c...</td>\n",
       "      <td>Your OpenAI API account has been funded</td>\n",
       "      <td>other</td>\n",
       "      <td>read only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                               body  \\\n",
       "0  194e90265a3c53fe  *ì§€ì†ê°€ëŠ¥ì›ì´ ì¶”ì²œí•˜ëŠ” 2025-1 ì§€ì†ê°€ëŠ¥ êµê³¼ëª© 25ì„ !*\\n\\nESG? ì§€ì†ê°€...   \n",
       "1  194e343333e5ff27                         \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "2  194e34039181a3c0  ì•ˆë…•í•˜ì„¸ìš”,\\n\\n\\n2024ë…„ ì—…ì í‰ê°€ë¥¼ ìœ„í•´ ì‹¤ì  ì—…ë°ì´íŠ¸ ìš”ì²­ì„ ë“œë¦½ë‹ˆë‹¤. ì´...   \n",
       "3  194e334940f6bad0  ì•ˆë…•í•˜ì„¸ìš”\\nì¹œí™˜ê²½ ë””ì§€í„¸ ì •ë³´ê³¼í•™ êµìœ¡ì—°êµ¬ë‹¨ í•¨ì´ì—´ì…ë‹ˆë‹¤.\\n\\nBK21 ëŒ€í•™ì›í˜...   \n",
       "4  194e325d62686180  Hi there,\\n\\nWe charged $5.20 to your credit c...   \n",
       "\n",
       "                                             subject  category         action  \n",
       "0           [ì§€ì†ê°€ëŠ¥ì›] ì§€ì†ê°€ëŠ¥ì›ì´ ì¶”ì²œí•˜ëŠ” 2025-1 ì§€ì†ê°€ëŠ¥ êµê³¼ëª© 25ì„ !     other      read only  \n",
       "1                    ICTëª…í’ˆì¸ì¬ì–‘ì„±ì‚¬ì—…ë‹¨ ë‰´ìŠ¤ë ˆí„° ê²¨ìš¸í˜¸ë¥¼ ë³´ë‚´ ë“œë¦½ë‹ˆë‹¤.  academic      read only  \n",
       "2            2024ë…„ ì—…ì í‰ê°€ë¥¼ ìœ„í•œ ì—°êµ¬ì‹¤ì  ì—…ë°ì´íŠ¸ ìš”ì²­ì˜ ê±´ (~2ì›” 7ì¼)  academic  action needed  \n",
       "3  [BK] 2025ë…„ 2ì›” 7ì¼(ê¸ˆ) BK21 ëŒ€í•™ì›í˜ì‹ ì‚¬ì—… ì˜ì–´ë…¼ë¬¸ì‘ì„±ë²• ì›Œí¬ìˆ ì•ˆ...  academic  action needed  \n",
       "4            Your OpenAI API account has been funded     other      read only  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_df = pd.read_csv(REFERNECE_CSV)\n",
    "print(\"len(reference_df):\", len(reference_df))\n",
    "reference_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary í‰ê°€\n",
    "\n",
    "1. ìš”ì•½ì „ ì›ë¬¸(ê³ ì •)\n",
    "2. REFERENCE(ê³ ì •)\n",
    "3. ìƒì„± ìš”ì•½ë¬¸\n",
    "\n",
    "Return\n",
    "ROUGE, BERT SCORE, G-EVAL(with summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(generated_summary_df): 48\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194e90265a3c53fe</td>\n",
       "      <td>ì§€ì†ê°€ëŠ¥ì›ì´ ì¶”ì²œí•˜ëŠ” 2025-1 í•™ê¸° ì§€ì†ê°€ëŠ¥ êµê³¼ëª© 25ì„  ì•ˆë‚´, ë§í¬ë¥¼ í†µí•´ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>194e343333e5ff27</td>\n",
       "      <td>ICTëª…í’ˆì¸ì¬ì–‘ì„±ì‚¬ì—…ë‹¨ ë‰´ìŠ¤ë ˆí„° ê²¨ìš¸í˜¸ë¥¼ ë³´ë‚´ ë“œë¦½ë‹ˆë‹¤.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>194e34039181a3c0</td>\n",
       "      <td>2024ë…„ ì—…ì í‰ê°€ë¥¼ ìœ„í•œ ì—°êµ¬ì‹¤ì  ì—…ë°ì´íŠ¸ ìš”ì²­, ë…¼ë¬¸, í•™íšŒ, íŠ¹í—ˆ, ì—°êµ¬ë¹„ ì‹¤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194e334940f6bad0</td>\n",
       "      <td>BK21 ëŒ€í•™ì›í˜ì‹ ì‚¬ì—… 2025ë…„ 1í•™ê¸° ëŒ€í•™ì› ë§ì¶¤í˜• ì—°êµ¬ì—­ëŸ‰ í”„ë¡œê·¸ë¨ - ì˜ì–´ë…¼...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>194e325d62686180</td>\n",
       "      <td>ì‹ ìš©ì¹´ë“œ(ëìë¦¬ 2043)ë¡œ 5.20ë‹¬ëŸ¬ê°€ ì²­êµ¬ë˜ì–´ OpenAI API í¬ë ˆë”§ ì”ì•¡...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                            summary\n",
       "0  194e90265a3c53fe  ì§€ì†ê°€ëŠ¥ì›ì´ ì¶”ì²œí•˜ëŠ” 2025-1 í•™ê¸° ì§€ì†ê°€ëŠ¥ êµê³¼ëª© 25ì„  ì•ˆë‚´, ë§í¬ë¥¼ í†µí•´ ...\n",
       "1  194e343333e5ff27                    ICTëª…í’ˆì¸ì¬ì–‘ì„±ì‚¬ì—…ë‹¨ ë‰´ìŠ¤ë ˆí„° ê²¨ìš¸í˜¸ë¥¼ ë³´ë‚´ ë“œë¦½ë‹ˆë‹¤.\n",
       "2  194e34039181a3c0  2024ë…„ ì—…ì í‰ê°€ë¥¼ ìœ„í•œ ì—°êµ¬ì‹¤ì  ì—…ë°ì´íŠ¸ ìš”ì²­, ë…¼ë¬¸, í•™íšŒ, íŠ¹í—ˆ, ì—°êµ¬ë¹„ ì‹¤...\n",
       "3  194e334940f6bad0  BK21 ëŒ€í•™ì›í˜ì‹ ì‚¬ì—… 2025ë…„ 1í•™ê¸° ëŒ€í•™ì› ë§ì¶¤í˜• ì—°êµ¬ì—­ëŸ‰ í”„ë¡œê·¸ë¨ - ì˜ì–´ë…¼...\n",
       "4  194e325d62686180  ì‹ ìš©ì¹´ë“œ(ëìë¦¬ 2043)ë¡œ 5.20ë‹¬ëŸ¬ê°€ ì²­êµ¬ë˜ì–´ OpenAI API í¬ë ˆë”§ ì”ì•¡..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_summary_df = pd.read_csv(GENERATED_SUMMARY_CSV)\n",
    "print(\"len(generated_summary_df):\", len(generated_summary_df))\n",
    "generated_summary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyejun\\github\\level4-nlp-finalproject-hackathon-nlp-06-lv3\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "g-eval start with **gpt-4**\n",
      "\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=The text is not provided, so I cannot evaluate the consistency of the summary.\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=3\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=4\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=4.5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=4.5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=2\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=2\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=3\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=3\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=2\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=2\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=The provided text does not allow me to evaluate the relevance of the summary as the source text and the summary are in different languages. I am unable to compare the summary to the email and identify the key points, essential details, and main purpose of the\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=2\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=3\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=4\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=4\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=1\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=3\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=2\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=4\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n"
     ]
    }
   ],
   "source": [
    "source_texts = reference_df[\"body\"].tolist()\n",
    "report_texts = generated_summary_df[\"summary\"].tolist()\n",
    "reference_texts = reference_df[\"subject\"].tolist()\n",
    "\n",
    "\n",
    "summary_results = evaluate_summary(source_texts, report_texts, reference_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SUMMARY Evaluation Results =====\n",
      "\n",
      "--- Summary Sample 1 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:0.9323, R:0.9488, F:0.9404\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 2 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=0.0000, coherence=1.0000, fluency=1.0000, relevance=3.0000\n",
      "\n",
      "--- Summary Sample 3 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:0.9181, R:0.9636, F:0.9403\n",
      "[G-EVAL] consistency=5.0000, coherence=4.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 4 ---\n",
      "[ROUGE] R1=(P:0.3333,R:0.8000,F:0.4706), R2=(P:0.0909,R:0.2500,F:0.1333), RL=(P:0.2500,R:0.6000,F:0.3529)\n",
      "[BERT] P:0.9066, R:0.9386, F:0.9223\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 5 ---\n",
      "[ROUGE] R1=(P:0.4000,R:0.2857,F:0.3333), R2=(P:0.2500,R:0.1667,F:0.2000), RL=(P:0.4000,R:0.2857,F:0.3333)\n",
      "[BERT] P:0.5310, R:0.6665, F:0.5911\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 6 ---\n",
      "[ROUGE] R1=(P:0.4000,R:0.2857,F:0.3333), R2=(P:0.2500,R:0.1667,F:0.2000), RL=(P:0.4000,R:0.2857,F:0.3333)\n",
      "[BERT] P:0.5322, R:0.6655, F:0.5914\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 7 ---\n",
      "[ROUGE] R1=(P:0.4000,R:0.2857,F:0.3333), R2=(P:0.2500,R:0.1667,F:0.2000), RL=(P:0.4000,R:0.2857,F:0.3333)\n",
      "[BERT] P:0.5325, R:0.6652, F:0.5915\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 8 ---\n",
      "[ROUGE] R1=(P:0.2000,R:0.3333,F:0.2500), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.2000,R:0.3333,F:0.2500)\n",
      "[BERT] P:0.8000, R:0.8679, F:0.8326\n",
      "[G-EVAL] consistency=5.0000, coherence=4.0000, fluency=1.0000, relevance=4.0000\n",
      "\n",
      "--- Summary Sample 9 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.6850, R:0.8665, F:0.7651\n",
      "[G-EVAL] consistency=5.0000, coherence=4.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 10 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.7745, R:0.8638, F:0.8167\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 11 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.7976, R:0.9067, F:0.8487\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 12 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.7976, R:0.9067, F:0.8487\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 13 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8175, R:0.8930, F:0.8536\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 14 ---\n",
      "[ROUGE] R1=(P:0.2000,R:1.0000,F:0.3333), R2=(P:0.1111,R:1.0000,F:0.2000), RL=(P:0.2000,R:1.0000,F:0.3333)\n",
      "[BERT] P:0.8199, R:0.8931, F:0.8549\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 15 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:0.5000,R:0.5000,F:0.5000), RL=(P:0.6667,R:0.6667,F:0.6667)\n",
      "[BERT] P:0.9147, R:0.9233, F:0.9190\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 16 ---\n",
      "[ROUGE] R1=(P:0.3333,R:1.0000,F:0.5000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.3333,R:1.0000,F:0.5000)\n",
      "[BERT] P:0.8772, R:0.9241, F:0.9000\n",
      "[G-EVAL] consistency=5.0000, coherence=2.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 17 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:0.9261, R:0.9671, F:0.9462\n",
      "[G-EVAL] consistency=5.0000, coherence=4.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 18 ---\n",
      "[ROUGE] R1=(P:0.0833,R:1.0000,F:0.1538), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0833,R:1.0000,F:0.1538)\n",
      "[BERT] P:0.8057, R:0.9040, F:0.8520\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 19 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8967, R:0.9152, F:0.9059\n",
      "[G-EVAL] consistency=5.0000, coherence=4.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 20 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8725, R:0.8960, F:0.8841\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 21 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.9159, R:0.9269, F:0.9214\n",
      "[G-EVAL] consistency=5.0000, coherence=4.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 22 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8153, R:0.8779, F:0.8455\n",
      "[G-EVAL] consistency=5.0000, coherence=2.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 23 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8580, R:0.8953, F:0.8763\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 24 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.9088, R:0.8725, F:0.8903\n",
      "[G-EVAL] consistency=1.0000, coherence=5.0000, fluency=1.0000, relevance=3.0000\n",
      "\n",
      "--- Summary Sample 25 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8850, R:0.9240, F:0.9041\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 26 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8443, R:0.8735, F:0.8587\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 27 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8923, R:0.8953, F:0.8938\n",
      "[G-EVAL] consistency=3.0000, coherence=4.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 28 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8643, R:0.8746, F:0.8694\n",
      "[G-EVAL] consistency=5.0000, coherence=2.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 29 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8808, R:0.9245, F:0.9021\n",
      "[G-EVAL] consistency=2.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 30 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.9215, R:0.9144, F:0.9179\n",
      "[G-EVAL] consistency=1.0000, coherence=4.0000, fluency=1.0000, relevance=0.0000\n",
      "\n",
      "--- Summary Sample 31 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8856, R:0.9232, F:0.9040\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 32 ---\n",
      "[ROUGE] R1=(P:0.1333,R:0.2500,F:0.1739), R2=(P:0.0714,R:0.1429,F:0.0952), RL=(P:0.1333,R:0.2500,F:0.1739)\n",
      "[BERT] P:0.6008, R:0.7306, F:0.6593\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=1.0000, relevance=2.0000\n",
      "\n",
      "--- Summary Sample 33 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8141, R:0.8469, F:0.8302\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 34 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.7734, R:0.8669, F:0.8175\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 35 ---\n",
      "[ROUGE] R1=(P:0.0400,R:1.0000,F:0.0769), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0400,R:1.0000,F:0.0769)\n",
      "[BERT] P:0.7730, R:0.8839, F:0.8248\n",
      "[G-EVAL] consistency=5.0000, coherence=1.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 36 ---\n",
      "[ROUGE] R1=(P:0.3636,R:1.0000,F:0.5333), R2=(P:0.2000,R:0.6667,F:0.3077), RL=(P:0.3636,R:1.0000,F:0.5333)\n",
      "[BERT] P:0.8485, R:0.8967, F:0.8719\n",
      "[G-EVAL] consistency=3.0000, coherence=5.0000, fluency=1.0000, relevance=4.0000\n",
      "\n",
      "--- Summary Sample 37 ---\n",
      "[ROUGE] R1=(P:0.5000,R:0.5000,F:0.5000), R2=(P:0.2857,R:0.2857,F:0.2857), RL=(P:0.5000,R:0.5000,F:0.5000)\n",
      "[BERT] P:0.8722, R:0.8885, F:0.8803\n",
      "[G-EVAL] consistency=5.0000, coherence=4.0000, fluency=1.0000, relevance=4.0000\n",
      "\n",
      "--- Summary Sample 38 ---\n",
      "[ROUGE] R1=(P:0.2222,R:1.0000,F:0.3636), R2=(P:0.1250,R:1.0000,F:0.2222), RL=(P:0.2222,R:1.0000,F:0.3636)\n",
      "[BERT] P:0.8569, R:0.9247, F:0.8895\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 39 ---\n",
      "[ROUGE] R1=(P:0.4000,R:1.0000,F:0.5714), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.2000,R:0.5000,F:0.2857)\n",
      "[BERT] P:0.8275, R:0.8779, F:0.8520\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=1.0000, relevance=1.0000\n",
      "\n",
      "--- Summary Sample 40 ---\n",
      "[ROUGE] R1=(P:0.2000,R:1.0000,F:0.3333), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.2000,R:1.0000,F:0.3333)\n",
      "[BERT] P:0.9000, R:0.9269, F:0.9133\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 41 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:0.8854, R:0.9282, F:0.9063\n",
      "[G-EVAL] consistency=5.0000, coherence=4.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 42 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8443, R:0.9571, F:0.8972\n",
      "[G-EVAL] consistency=5.0000, coherence=4.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 43 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:0.8662, R:0.9289, F:0.8964\n",
      "[G-EVAL] consistency=3.0000, coherence=4.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 44 ---\n",
      "[ROUGE] R1=(P:1.0000,R:0.5000,F:0.6667), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:1.0000,R:0.5000,F:0.6667)\n",
      "[BERT] P:0.9034, R:0.9197, F:0.9114\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 45 ---\n",
      "[ROUGE] R1=(P:0.1364,R:0.6000,F:0.2222), R2=(P:0.0952,R:0.5000,F:0.1600), RL=(P:0.1364,R:0.6000,F:0.2222)\n",
      "[BERT] P:0.5453, R:0.8059, F:0.6505\n",
      "[G-EVAL] consistency=5.0000, coherence=2.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 46 ---\n",
      "[ROUGE] R1=(P:0.0870,R:0.2000,F:0.1212), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0870,R:0.2000,F:0.1212)\n",
      "[BERT] P:0.5253, R:0.6833, F:0.5940\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 47 ---\n",
      "[ROUGE] R1=(P:0.7143,R:1.0000,F:0.8333), R2=(P:0.5000,R:0.7500,F:0.6000), RL=(P:0.7143,R:1.0000,F:0.8333)\n",
      "[BERT] P:0.8526, R:0.8617, F:0.8572\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 48 ---\n",
      "[ROUGE] R1=(P:0.5556,R:0.8333,F:0.6667), R2=(P:0.3750,R:0.6000,F:0.4615), RL=(P:0.5556,R:0.8333,F:0.6667)\n",
      "[BERT] P:0.8167, R:0.8802, F:0.8473\n",
      "[G-EVAL] consistency=4.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "===== Averages =====\n",
      "\n",
      "[ROUGE Avg]\n",
      "  ROUGE-1: P=0.2646, R=0.1707, F1=0.3925\n",
      "  ROUGE-2: P=0.0000, R=0.0000, F1=0.0000\n",
      "  ROUGE-L: P=0.0000, R=0.0000, F1=0.0000\n",
      "\n",
      "[BERT Avg]\n",
      "  Precision: 0.8191, Recall: 0.8810, F1: 0.8476\n",
      "\n",
      "[G-EVAL Avg]\n",
      "  consistency=4.3542, coherence=4.1667, fluency=1.0000, relevance=4.6042\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_results(summary_results, eval_type=\"summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë¶„ë¥˜ í‰ê°€\n",
    "\n",
    "1. ë¼ë²¨ Ground truth(ê³ ì •)\n",
    "2. ì—¬ëŸ¬ë²ˆ ê²°ê³¼(Në²ˆ) ê°€ì§€ê³  ìˆëŠ” ë¦¬ìŠ¤íŠ¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(generated_category_df): 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category1</th>\n",
       "      <th>category2</th>\n",
       "      <th>category3</th>\n",
       "      <th>category4</th>\n",
       "      <th>category5</th>\n",
       "      <th>action1</th>\n",
       "      <th>action2</th>\n",
       "      <th>action3</th>\n",
       "      <th>action4</th>\n",
       "      <th>action5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194e90265a3c53fe</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>read only</td>\n",
       "      <td>read only</td>\n",
       "      <td>read only</td>\n",
       "      <td>read only</td>\n",
       "      <td>read only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>194e34039181a3c0</td>\n",
       "      <td>academic</td>\n",
       "      <td>academic</td>\n",
       "      <td>academic</td>\n",
       "      <td>academic</td>\n",
       "      <td>academic</td>\n",
       "      <td>action needed</td>\n",
       "      <td>action needed</td>\n",
       "      <td>action needed</td>\n",
       "      <td>action needed</td>\n",
       "      <td>action needed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id category1 category2 category3 category4 category5  \\\n",
       "0  194e90265a3c53fe     other     other     other     other     other   \n",
       "1  194e34039181a3c0  academic  academic  academic  academic  academic   \n",
       "\n",
       "         action1        action2        action3        action4        action5  \n",
       "0      read only      read only      read only      read only      read only  \n",
       "1  action needed  action needed  action needed  action needed  action needed  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_category_df = pd.read_csv(GENERATED_CATEGORY_CSV)\n",
    "print(\"len(generated_category_df):\", len(generated_category_df))\n",
    "generated_category_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyejun\\github\\level4-nlp-finalproject-hackathon-nlp-06-lv3\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hyejun\\github\\level4-nlp-finalproject-hackathon-nlp-06-lv3\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Confusion Matrix ì €ì¥ ì™„ë£Œ: evaluation/classification/figure\\academic_confusion_matrix.png\n",
      "âœ… Confusion Matrix ì €ì¥ ì™„ë£Œ: evaluation/classification/figure\\other_confusion_matrix.png\n",
      "\n",
      "Correctness\n",
      "ğŸ¯ ì „ì²´ ì •í™•ë„: 1.0000\n",
      "ğŸ¯ academic ì •í™•ë„: 1.0000\n",
      "ğŸ¯ other ì •í™•ë„: 1.0000\n",
      "\n",
      "Consistency\n",
      "ğŸ“Š Ground Truth ë³„ ìš”ì•½ëœ í‰ê°€ ë©”íŠ¸ë¦­\n",
      "  Ground Truth  Entropy  Diversity Index  Chi-Square p-value  Accuracy  \\\n",
      "0     academic      0.0              0.2                 1.0       1.0   \n",
      "1        other      0.0              0.2                 1.0       1.0   \n",
      "\n",
      "   Cramer's V  \n",
      "0         0.0  \n",
      "1         0.0  \n",
      "\n",
      "=== Overall Multiclass Confusion Matrix ===\n",
      "Labels: ['academic', 'other']\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "===========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyejun\\github\\level4-nlp-finalproject-hackathon-nlp-06-lv3\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hyejun\\github\\level4-nlp-finalproject-hackathon-nlp-06-lv3\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "category_df_manager = DataFrameManager(5, ClassificationType.CATEGORY)\n",
    "for index, row in generated_category_df.iterrows():\n",
    "    results = [row[\"category1\"], row[\"category2\"], row[\"category3\"], row[\"category4\"], row[\"category5\"]]\n",
    "    category_df_manager.update_eval_df(row[\"id\"], results, reference_df.loc[index, \"category\"])\n",
    "\n",
    "category_df_manager.print_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Confusion Matrix ì €ì¥ ì™„ë£Œ: evaluation/classification/figure\\read only_confusion_matrix.png\n",
      "\n",
      "Correctness\n",
      "ğŸ¯ ì „ì²´ ì •í™•ë„: 0.5000\n",
      "ğŸ¯ read only ì •í™•ë„: 0.5000\n",
      "\n",
      "Consistency\n",
      "ğŸ“Š Ground Truth ë³„ ìš”ì•½ëœ í‰ê°€ ë©”íŠ¸ë¦­\n",
      "  Ground Truth   Entropy  Diversity Index  Chi-Square p-value  Accuracy  \\\n",
      "0    read only  0.693147              0.2                   1       0.5   \n",
      "\n",
      "   Cramer's V  \n",
      "0         0.0  \n",
      "\n",
      "=== Overall Multiclass Confusion Matrix ===\n",
      "Labels: ['action needed', 'read only']\n",
      "[[0 0]\n",
      " [5 5]]\n",
      "===========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyejun\\github\\level4-nlp-finalproject-hackathon-nlp-06-lv3\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "action_df_manager = DataFrameManager(5, ClassificationType.ACTION)\n",
    "for index, row in generated_category_df.iterrows():\n",
    "    results = [row[\"action1\"], row[\"action2\"], row[\"action3\"], row[\"action4\"], row[\"action5\"]]\n",
    "    action_df_manager.update_eval_df(row[\"id\"], results, reference_df.loc[index, \"action\"])\n",
    "\n",
    "action_df_manager.print_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìµœì¢… report í‰ê°€\n",
    "\n",
    "1. ìš”ì•½ì „ ì›ë¬¸(ë©”ì¼ ìš”ì•½ë¬¸ concat)\n",
    "2. ìƒì„± ìš”ì•½ë¬¸\n",
    "\n",
    "Return\n",
    "G-EVAL(with final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(generated_report_df): 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. ì§€ì†ê°€ëŠ¥ì›ì´ ì¶”ì²œí•˜ëŠ” 2025-1 í•™ê¸° ì§€ì†ê°€ëŠ¥ êµê³¼ëª© 25ì„  ì•ˆë‚´.\\r\\n2...</td>\n",
       "      <td>1. ì§€ì†ê°€ëŠ¥ì›ì´ ì¶”ì²œí•˜ëŠ” 2025-1 í•™ê¸° ì§€ì†ê°€ëŠ¥ êµê³¼ëª© 25ì„  ì•ˆë‚´.\\r\\n2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  1. ì§€ì†ê°€ëŠ¥ì›ì´ ì¶”ì²œí•˜ëŠ” 2025-1 í•™ê¸° ì§€ì†ê°€ëŠ¥ êµê³¼ëª© 25ì„  ì•ˆë‚´.\\r\\n2...   \n",
       "\n",
       "                                              report  \n",
       "0  1. ì§€ì†ê°€ëŠ¥ì›ì´ ì¶”ì²œí•˜ëŠ” 2025-1 í•™ê¸° ì§€ì†ê°€ëŠ¥ êµê³¼ëª© 25ì„  ì•ˆë‚´.\\r\\n2...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_report_df = pd.read_csv(GENERATED_REPORT_CSV)\n",
    "print(\"len(generated_report_df):\", len(generated_report_df))\n",
    "generated_report_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "g-eval start with **gpt-4**\n",
      "\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=4\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=4\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "results[\"g-eval\"] = calculate_g_eval(\n",
    "    source_texts=generated_report_df[\"source\"].tolist(),\n",
    "    generated_texts=generated_report_df[\"report\"].tolist(),\n",
    "    eval_type=\"report\",\n",
    "    model_name=Config.config[\"summary\"][\"g_eval\"][\"openai_model\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== REPORT Evaluation Results =====\n",
      "\n",
      "--- Report Sample 1 ---\n",
      "[G-EVAL] consistency=4.0000, coherence=4.0000, fluency=1.0000, relevance=4.0000\n",
      "\n",
      "===== Averages =====\n",
      "\n",
      "[G-EVAL Avg]\n",
      "  consistency=4.0000, coherence=4.0000, fluency=1.0000, relevance=4.0000\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_results(results, eval_type=\"report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
