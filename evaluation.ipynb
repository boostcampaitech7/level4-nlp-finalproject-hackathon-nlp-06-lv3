{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result í‰ê°€ ìŠ¤í¬ë¦½íŠ¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATED_SUMMARY_CSV = \"evaluation/data/generated_summary.csv\"\n",
    "GENERATED_CATEGORY_CSV = \"evaluation/data/generated_category.csv\"\n",
    "GENERATED_REPORT_CSV = \"evaluation/data/generated_report.csv\"\n",
    "\n",
    "REFERNECE_CSV = \"evaluation/data/reference.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import ë° í™˜ê²½ ì£¼ì…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from agents.classification.classification_type import ClassificationType\n",
    "from evaluation.classification.dataframe_manager import DataFrameManager\n",
    "from evaluation.evaluation_summary import evaluate_summary\n",
    "from evaluation.gpt_eval import calculate_g_eval\n",
    "from evaluation.result_printer import print_evaluation_results\n",
    "from utils.configuration import Config\n",
    "\n",
    "load_dotenv()\n",
    "Config.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(reference_df): 48\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>body</th>\n",
       "      <th>subject</th>\n",
       "      <th>category</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194e90265a3c53fe</td>\n",
       "      <td>*ì§€ì†ê°€ëŠ¥ì›ì´ ì¶”ì²œí•˜ëŠ” 2025-1 ì§€ì†ê°€ëŠ¥ êµê³¼ëª© 25ì„ !*\\n\\nESG? ì§€ì†ê°€...</td>\n",
       "      <td>[ì§€ì†ê°€ëŠ¥ì›] ì§€ì†ê°€ëŠ¥ì›ì´ ì¶”ì²œí•˜ëŠ” 2025-1 ì§€ì†ê°€ëŠ¥ êµê³¼ëª© 25ì„ !</td>\n",
       "      <td>other</td>\n",
       "      <td>read only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>194e343333e5ff27</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</td>\n",
       "      <td>ICTëª…í’ˆì¸ì¬ì–‘ì„±ì‚¬ì—…ë‹¨ ë‰´ìŠ¤ë ˆí„° ê²¨ìš¸í˜¸ë¥¼ ë³´ë‚´ ë“œë¦½ë‹ˆë‹¤.</td>\n",
       "      <td>academic</td>\n",
       "      <td>read only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>194e34039181a3c0</td>\n",
       "      <td>ì•ˆë…•í•˜ì„¸ìš”,\\n\\n\\n2024ë…„ ì—…ì í‰ê°€ë¥¼ ìœ„í•´ ì‹¤ì  ì—…ë°ì´íŠ¸ ìš”ì²­ì„ ë“œë¦½ë‹ˆë‹¤. ì´...</td>\n",
       "      <td>2024ë…„ ì—…ì í‰ê°€ë¥¼ ìœ„í•œ ì—°êµ¬ì‹¤ì  ì—…ë°ì´íŠ¸ ìš”ì²­ì˜ ê±´ (~2ì›” 7ì¼)</td>\n",
       "      <td>academic</td>\n",
       "      <td>action needed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194e334940f6bad0</td>\n",
       "      <td>ì•ˆë…•í•˜ì„¸ìš”\\nì¹œí™˜ê²½ ë””ì§€í„¸ ì •ë³´ê³¼í•™ êµìœ¡ì—°êµ¬ë‹¨ í•¨ì´ì—´ì…ë‹ˆë‹¤.\\n\\nBK21 ëŒ€í•™ì›í˜...</td>\n",
       "      <td>[BK] 2025ë…„ 2ì›” 7ì¼(ê¸ˆ) BK21 ëŒ€í•™ì›í˜ì‹ ì‚¬ì—… ì˜ì–´ë…¼ë¬¸ì‘ì„±ë²• ì›Œí¬ìˆ ì•ˆ...</td>\n",
       "      <td>academic</td>\n",
       "      <td>action needed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>194e325d62686180</td>\n",
       "      <td>Hi there,\\n\\nWe charged $5.20 to your credit c...</td>\n",
       "      <td>Your OpenAI API account has been funded</td>\n",
       "      <td>other</td>\n",
       "      <td>read only</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                               body  \\\n",
       "0  194e90265a3c53fe  *ì§€ì†ê°€ëŠ¥ì›ì´ ì¶”ì²œí•˜ëŠ” 2025-1 ì§€ì†ê°€ëŠ¥ êµê³¼ëª© 25ì„ !*\\n\\nESG? ì§€ì†ê°€...   \n",
       "1  194e343333e5ff27                         \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n   \n",
       "2  194e34039181a3c0  ì•ˆë…•í•˜ì„¸ìš”,\\n\\n\\n2024ë…„ ì—…ì í‰ê°€ë¥¼ ìœ„í•´ ì‹¤ì  ì—…ë°ì´íŠ¸ ìš”ì²­ì„ ë“œë¦½ë‹ˆë‹¤. ì´...   \n",
       "3  194e334940f6bad0  ì•ˆë…•í•˜ì„¸ìš”\\nì¹œí™˜ê²½ ë””ì§€í„¸ ì •ë³´ê³¼í•™ êµìœ¡ì—°êµ¬ë‹¨ í•¨ì´ì—´ì…ë‹ˆë‹¤.\\n\\nBK21 ëŒ€í•™ì›í˜...   \n",
       "4  194e325d62686180  Hi there,\\n\\nWe charged $5.20 to your credit c...   \n",
       "\n",
       "                                             subject  category         action  \n",
       "0           [ì§€ì†ê°€ëŠ¥ì›] ì§€ì†ê°€ëŠ¥ì›ì´ ì¶”ì²œí•˜ëŠ” 2025-1 ì§€ì†ê°€ëŠ¥ êµê³¼ëª© 25ì„ !     other      read only  \n",
       "1                    ICTëª…í’ˆì¸ì¬ì–‘ì„±ì‚¬ì—…ë‹¨ ë‰´ìŠ¤ë ˆí„° ê²¨ìš¸í˜¸ë¥¼ ë³´ë‚´ ë“œë¦½ë‹ˆë‹¤.  academic      read only  \n",
       "2            2024ë…„ ì—…ì í‰ê°€ë¥¼ ìœ„í•œ ì—°êµ¬ì‹¤ì  ì—…ë°ì´íŠ¸ ìš”ì²­ì˜ ê±´ (~2ì›” 7ì¼)  academic  action needed  \n",
       "3  [BK] 2025ë…„ 2ì›” 7ì¼(ê¸ˆ) BK21 ëŒ€í•™ì›í˜ì‹ ì‚¬ì—… ì˜ì–´ë…¼ë¬¸ì‘ì„±ë²• ì›Œí¬ìˆ ì•ˆ...  academic  action needed  \n",
       "4            Your OpenAI API account has been funded     other      read only  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_df = pd.read_csv(REFERNECE_CSV)\n",
    "print(\"len(reference_df):\", len(reference_df))\n",
    "reference_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary í‰ê°€\n",
    "\n",
    "1. ìš”ì•½ì „ ì›ë¬¸(ê³ ì •)\n",
    "2. REFERENCE(ê³ ì •)\n",
    "3. ìƒì„± ìš”ì•½ë¬¸\n",
    "\n",
    "Return\n",
    "ROUGE, BERT SCORE, G-EVAL(with summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(generated_summary_df): 48\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194e90265a3c53fe</td>\n",
       "      <td>ì§€ì†ê°€ëŠ¥ì›ì´ ì¶”ì²œí•˜ëŠ” 2025-1 í•™ê¸° ì§€ì†ê°€ëŠ¥ êµê³¼ëª© 25ì„  ì•ˆë‚´, ë§í¬ë¥¼ í†µí•´ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>194e343333e5ff27</td>\n",
       "      <td>ICTëª…í’ˆì¸ì¬ì–‘ì„±ì‚¬ì—…ë‹¨ ë‰´ìŠ¤ë ˆí„° ê²¨ìš¸í˜¸ë¥¼ ë³´ë‚´ ë“œë¦½ë‹ˆë‹¤.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>194e34039181a3c0</td>\n",
       "      <td>2024ë…„ ì—…ì í‰ê°€ë¥¼ ìœ„í•œ ì—°êµ¬ì‹¤ì  ì—…ë°ì´íŠ¸ ìš”ì²­, ë…¼ë¬¸, í•™íšŒ, íŠ¹í—ˆ, ì—°êµ¬ë¹„ ì‹¤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194e334940f6bad0</td>\n",
       "      <td>BK21 ëŒ€í•™ì›í˜ì‹ ì‚¬ì—… 2025ë…„ 1í•™ê¸° ëŒ€í•™ì› ë§ì¶¤í˜• ì—°êµ¬ì—­ëŸ‰ í”„ë¡œê·¸ë¨ - ì˜ì–´ë…¼...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>194e325d62686180</td>\n",
       "      <td>ì‹ ìš©ì¹´ë“œ(ëìë¦¬ 2043)ë¡œ 5.20ë‹¬ëŸ¬ê°€ ì²­êµ¬ë˜ì–´ OpenAI API í¬ë ˆë”§ ì”ì•¡...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                            summary\n",
       "0  194e90265a3c53fe  ì§€ì†ê°€ëŠ¥ì›ì´ ì¶”ì²œí•˜ëŠ” 2025-1 í•™ê¸° ì§€ì†ê°€ëŠ¥ êµê³¼ëª© 25ì„  ì•ˆë‚´, ë§í¬ë¥¼ í†µí•´ ...\n",
       "1  194e343333e5ff27                    ICTëª…í’ˆì¸ì¬ì–‘ì„±ì‚¬ì—…ë‹¨ ë‰´ìŠ¤ë ˆí„° ê²¨ìš¸í˜¸ë¥¼ ë³´ë‚´ ë“œë¦½ë‹ˆë‹¤.\n",
       "2  194e34039181a3c0  2024ë…„ ì—…ì í‰ê°€ë¥¼ ìœ„í•œ ì—°êµ¬ì‹¤ì  ì—…ë°ì´íŠ¸ ìš”ì²­, ë…¼ë¬¸, í•™íšŒ, íŠ¹í—ˆ, ì—°êµ¬ë¹„ ì‹¤...\n",
       "3  194e334940f6bad0  BK21 ëŒ€í•™ì›í˜ì‹ ì‚¬ì—… 2025ë…„ 1í•™ê¸° ëŒ€í•™ì› ë§ì¶¤í˜• ì—°êµ¬ì—­ëŸ‰ í”„ë¡œê·¸ë¨ - ì˜ì–´ë…¼...\n",
       "4  194e325d62686180  ì‹ ìš©ì¹´ë“œ(ëìë¦¬ 2043)ë¡œ 5.20ë‹¬ëŸ¬ê°€ ì²­êµ¬ë˜ì–´ OpenAI API í¬ë ˆë”§ ì”ì•¡..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_summary_df = pd.read_csv(GENERATED_SUMMARY_CSV)\n",
    "print(\"len(generated_summary_df):\", len(generated_summary_df))\n",
    "generated_summary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyejun\\github\\level4-nlp-finalproject-hackathon-nlp-06-lv3\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "g-eval start with **gpt-4**\n",
      "\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=This example doesn't provide enough information to evaluate. The source text and the summary are missing.\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=3\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=3\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=2\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=4\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=3\n",
      "[G-EVAL] aspect=coherence, gpt_text=2\n",
      "[G-EVAL] aspect=fluency, gpt_text=3\n",
      "[G-EVAL] aspect=relevance, gpt_text=2\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=3\n",
      "[G-EVAL] aspect=coherence, gpt_text=2\n",
      "[G-EVAL] aspect=fluency, gpt_text=3\n",
      "[G-EVAL] aspect=relevance, gpt_text=2\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=3\n",
      "[G-EVAL] aspect=coherence, gpt_text=2\n",
      "[G-EVAL] aspect=fluency, gpt_text=3\n",
      "[G-EVAL] aspect=relevance, gpt_text=2\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=3\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=1\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=2\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=2\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=1\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=1\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=1\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=3\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=1\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=1\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=1\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=3\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=3\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=This is difficult to rate without the full context and translation of the email and summary. However, assuming the summary accurately reflects the content of the email, I would rate it a 4. It seems to capture the main topic of the email but may\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=1\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=1\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=3\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=1\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=This email is not in English, so I am unable to provide a relevance rating.\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=1\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=1\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=3\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=1\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=1\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=3\n",
      "[G-EVAL] aspect=relevance, gpt_text=1\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=3\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=1\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=1\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=3\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=3\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=3\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=3\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=1\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=1\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=2\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=1\n",
      "[G-EVAL] aspect=fluency, gpt_text=3\n",
      "[G-EVAL] aspect=relevance, gpt_text=1\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=4\n",
      "[G-EVAL] aspect=fluency, gpt_text=3\n",
      "[G-EVAL] aspect=relevance, gpt_text=5\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=5\n",
      "[G-EVAL] aspect=coherence, gpt_text=3\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=4\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=1\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=1\n"
     ]
    }
   ],
   "source": [
    "source_texts = reference_df[\"body\"].tolist()\n",
    "report_texts = generated_summary_df[\"summary\"].tolist()\n",
    "reference_texts = reference_df[\"subject\"].tolist()\n",
    "\n",
    "\n",
    "summary_results = evaluate_summary(source_texts, report_texts, reference_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== SUMMARY Evaluation Results =====\n",
      "\n",
      "--- Summary Sample 1 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 2 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=1.0000, relevance=0.0000\n",
      "\n",
      "--- Summary Sample 3 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=5.0000, coherence=3.0000, fluency=1.0000, relevance=3.0000\n",
      "\n",
      "--- Summary Sample 4 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=2.0000, coherence=5.0000, fluency=1.0000, relevance=4.0000\n",
      "\n",
      "--- Summary Sample 5 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=3.0000, coherence=2.0000, fluency=3.0000, relevance=2.0000\n",
      "\n",
      "--- Summary Sample 6 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=3.0000, coherence=2.0000, fluency=3.0000, relevance=2.0000\n",
      "\n",
      "--- Summary Sample 7 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=3.0000, coherence=2.0000, fluency=3.0000, relevance=2.0000\n",
      "\n",
      "--- Summary Sample 8 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=3.0000, fluency=1.0000, relevance=1.0000\n",
      "\n",
      "--- Summary Sample 9 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=1.0000, relevance=2.0000\n",
      "\n",
      "--- Summary Sample 10 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=5.0000, coherence=4.0000, fluency=1.0000, relevance=2.0000\n",
      "\n",
      "--- Summary Sample 11 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=1.0000, relevance=1.0000\n",
      "\n",
      "--- Summary Sample 12 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=1.0000, relevance=1.0000\n",
      "\n",
      "--- Summary Sample 13 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=1.0000, relevance=1.0000\n",
      "\n",
      "--- Summary Sample 14 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 15 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=5.0000, coherence=1.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 16 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=5.0000, coherence=3.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 17 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=5.0000, coherence=1.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 18 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=1.0000, relevance=1.0000\n",
      "\n",
      "--- Summary Sample 19 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=1.0000, relevance=1.0000\n",
      "\n",
      "--- Summary Sample 20 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=1.0000, relevance=1.0000\n",
      "\n",
      "--- Summary Sample 21 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=5.0000, coherence=3.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 22 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=3.0000, fluency=1.0000, relevance=4.0000\n",
      "\n",
      "--- Summary Sample 23 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=1.0000, relevance=1.0000\n",
      "\n",
      "--- Summary Sample 24 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=1.0000, relevance=1.0000\n",
      "\n",
      "--- Summary Sample 25 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=5.0000, coherence=3.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 26 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=1.0000, relevance=1.0000\n",
      "\n",
      "--- Summary Sample 27 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=1.0000, relevance=0.0000\n",
      "\n",
      "--- Summary Sample 28 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=1.0000, relevance=1.0000\n",
      "\n",
      "--- Summary Sample 29 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=1.0000, relevance=1.0000\n",
      "\n",
      "--- Summary Sample 30 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=3.0000, fluency=1.0000, relevance=1.0000\n",
      "\n",
      "--- Summary Sample 31 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=1.0000, relevance=1.0000\n",
      "\n",
      "--- Summary Sample 32 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=3.0000, relevance=1.0000\n",
      "\n",
      "--- Summary Sample 33 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=3.0000, coherence=1.0000, fluency=1.0000, relevance=1.0000\n",
      "\n",
      "--- Summary Sample 34 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=1.0000, relevance=1.0000\n",
      "\n",
      "--- Summary Sample 35 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=1.0000, relevance=3.0000\n",
      "\n",
      "--- Summary Sample 36 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=3.0000, fluency=1.0000, relevance=3.0000\n",
      "\n",
      "--- Summary Sample 37 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 38 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 39 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 40 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=4.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 41 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=3.0000, fluency=1.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 42 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=1.0000, relevance=1.0000\n",
      "\n",
      "--- Summary Sample 43 ---\n",
      "[ROUGE] R1=(P:0.0000,R:0.0000,F:0.0000), R2=(P:0.0000,R:0.0000,F:0.0000), RL=(P:0.0000,R:0.0000,F:0.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=1.0000, relevance=1.0000\n",
      "\n",
      "--- Summary Sample 44 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=5.0000, coherence=5.0000, fluency=1.0000, relevance=2.0000\n",
      "\n",
      "--- Summary Sample 45 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=1.0000, fluency=3.0000, relevance=1.0000\n",
      "\n",
      "--- Summary Sample 46 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=5.0000, coherence=4.0000, fluency=3.0000, relevance=5.0000\n",
      "\n",
      "--- Summary Sample 47 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=5.0000, coherence=3.0000, fluency=1.0000, relevance=4.0000\n",
      "\n",
      "--- Summary Sample 48 ---\n",
      "[ROUGE] R1=(P:1.0000,R:1.0000,F:1.0000), R2=(P:1.0000,R:1.0000,F:1.0000), RL=(P:1.0000,R:1.0000,F:1.0000)\n",
      "[BERT] P:1.0000, R:1.0000, F:1.0000\n",
      "[G-EVAL] consistency=1.0000, coherence=5.0000, fluency=1.0000, relevance=1.0000\n",
      "\n",
      "===== Averages =====\n",
      "\n",
      "[ROUGE Avg]\n",
      "  ROUGE-1: P=0.7292, R=0.4792, F1=0.7292\n",
      "  ROUGE-2: P=0.0000, R=0.0000, F1=0.0000\n",
      "  ROUGE-L: P=0.0000, R=0.0000, F1=0.0000\n",
      "\n",
      "[BERT Avg]\n",
      "  Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "\n",
      "[G-EVAL Avg]\n",
      "  consistency=2.4375, coherence=2.3333, fluency=1.2500, relevance=2.4792\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_results(summary_results, eval_type=\"summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë¶„ë¥˜ í‰ê°€\n",
    "\n",
    "1. ë¼ë²¨ Ground truth(ê³ ì •)\n",
    "2. ì—¬ëŸ¬ë²ˆ ê²°ê³¼(Në²ˆ) ê°€ì§€ê³  ìˆëŠ” ë¦¬ìŠ¤íŠ¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(generated_category_df): 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category1</th>\n",
       "      <th>category2</th>\n",
       "      <th>category3</th>\n",
       "      <th>category4</th>\n",
       "      <th>category5</th>\n",
       "      <th>action1</th>\n",
       "      <th>action2</th>\n",
       "      <th>action3</th>\n",
       "      <th>action4</th>\n",
       "      <th>action5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>194e90265a3c53fe</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>read only</td>\n",
       "      <td>read only</td>\n",
       "      <td>read only</td>\n",
       "      <td>read only</td>\n",
       "      <td>read only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>194e34039181a3c0</td>\n",
       "      <td>academic</td>\n",
       "      <td>academic</td>\n",
       "      <td>academic</td>\n",
       "      <td>academic</td>\n",
       "      <td>academic</td>\n",
       "      <td>action needed</td>\n",
       "      <td>action needed</td>\n",
       "      <td>action needed</td>\n",
       "      <td>action needed</td>\n",
       "      <td>action needed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id category1 category2 category3 category4 category5  \\\n",
       "0  194e90265a3c53fe     other     other     other     other     other   \n",
       "1  194e34039181a3c0  academic  academic  academic  academic  academic   \n",
       "\n",
       "         action1        action2        action3        action4        action5  \n",
       "0      read only      read only      read only      read only      read only  \n",
       "1  action needed  action needed  action needed  action needed  action needed  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_category_df = pd.read_csv(GENERATED_CATEGORY_CSV)\n",
    "print(\"len(generated_category_df):\", len(generated_category_df))\n",
    "generated_category_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyejun\\github\\level4-nlp-finalproject-hackathon-nlp-06-lv3\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hyejun\\github\\level4-nlp-finalproject-hackathon-nlp-06-lv3\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Confusion Matrix ì €ì¥ ì™„ë£Œ: evaluation/classification/figure\\academic_confusion_matrix.png\n",
      "âœ… Confusion Matrix ì €ì¥ ì™„ë£Œ: evaluation/classification/figure\\other_confusion_matrix.png\n",
      "\n",
      "Correctness\n",
      "ğŸ¯ ì „ì²´ ì •í™•ë„: 1.0000\n",
      "ğŸ¯ academic ì •í™•ë„: 1.0000\n",
      "ğŸ¯ other ì •í™•ë„: 1.0000\n",
      "\n",
      "Consistency\n",
      "ğŸ“Š Ground Truth ë³„ ìš”ì•½ëœ í‰ê°€ ë©”íŠ¸ë¦­\n",
      "  Ground Truth  Entropy  Diversity Index  Chi-Square p-value  Accuracy  \\\n",
      "0     academic      0.0              0.2                 1.0       1.0   \n",
      "1        other      0.0              0.2                 1.0       1.0   \n",
      "\n",
      "   Cramer's V  \n",
      "0         0.0  \n",
      "1         0.0  \n",
      "\n",
      "=== Overall Multiclass Confusion Matrix ===\n",
      "Labels: ['academic', 'other']\n",
      "[[5 0]\n",
      " [0 5]]\n",
      "===========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyejun\\github\\level4-nlp-finalproject-hackathon-nlp-06-lv3\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "c:\\Users\\hyejun\\github\\level4-nlp-finalproject-hackathon-nlp-06-lv3\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "category_df_manager = DataFrameManager(5, ClassificationType.CATEGORY)\n",
    "for index, row in generated_category_df.iterrows():\n",
    "    results = [row[\"category1\"], row[\"category2\"], row[\"category3\"], row[\"category4\"], row[\"category5\"]]\n",
    "    category_df_manager.update_eval_df(row[\"id\"], results, reference_df.loc[index, \"category\"])\n",
    "\n",
    "category_df_manager.print_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Confusion Matrix ì €ì¥ ì™„ë£Œ: evaluation/classification/figure\\read only_confusion_matrix.png\n",
      "\n",
      "Correctness\n",
      "ğŸ¯ ì „ì²´ ì •í™•ë„: 0.5000\n",
      "ğŸ¯ read only ì •í™•ë„: 0.5000\n",
      "\n",
      "Consistency\n",
      "ğŸ“Š Ground Truth ë³„ ìš”ì•½ëœ í‰ê°€ ë©”íŠ¸ë¦­\n",
      "  Ground Truth   Entropy  Diversity Index  Chi-Square p-value  Accuracy  \\\n",
      "0    read only  0.693147              0.2                   1       0.5   \n",
      "\n",
      "   Cramer's V  \n",
      "0         0.0  \n",
      "\n",
      "=== Overall Multiclass Confusion Matrix ===\n",
      "Labels: ['action needed', 'read only']\n",
      "[[0 0]\n",
      " [5 5]]\n",
      "===========================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyejun\\github\\level4-nlp-finalproject-hackathon-nlp-06-lv3\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "action_df_manager = DataFrameManager(5, ClassificationType.ACTION)\n",
    "for index, row in generated_category_df.iterrows():\n",
    "    results = [row[\"action1\"], row[\"action2\"], row[\"action3\"], row[\"action4\"], row[\"action5\"]]\n",
    "    action_df_manager.update_eval_df(row[\"id\"], results, reference_df.loc[index, \"action\"])\n",
    "\n",
    "action_df_manager.print_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìµœì¢… report í‰ê°€\n",
    "\n",
    "1. ìš”ì•½ì „ ì›ë¬¸(ë©”ì¼ ìš”ì•½ë¬¸ concat)\n",
    "2. ìƒì„± ìš”ì•½ë¬¸\n",
    "\n",
    "Return\n",
    "G-EVAL(with final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(generated_report_df): 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. ì§€ì†ê°€ëŠ¥ì›ì´ ì¶”ì²œí•˜ëŠ” 2025-1 í•™ê¸° ì§€ì†ê°€ëŠ¥ êµê³¼ëª© 25ì„  ì•ˆë‚´.\\r\\n2...</td>\n",
       "      <td>1. ì§€ì†ê°€ëŠ¥ì›ì´ ì¶”ì²œí•˜ëŠ” 2025-1 í•™ê¸° ì§€ì†ê°€ëŠ¥ êµê³¼ëª© 25ì„  ì•ˆë‚´.\\r\\n2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  1. ì§€ì†ê°€ëŠ¥ì›ì´ ì¶”ì²œí•˜ëŠ” 2025-1 í•™ê¸° ì§€ì†ê°€ëŠ¥ êµê³¼ëª© 25ì„  ì•ˆë‚´.\\r\\n2...   \n",
       "\n",
       "                                              report  \n",
       "0  1. ì§€ì†ê°€ëŠ¥ì›ì´ ì¶”ì²œí•˜ëŠ” 2025-1 í•™ê¸° ì§€ì†ê°€ëŠ¥ êµê³¼ëª© 25ì„  ì•ˆë‚´.\\r\\n2...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_report_df = pd.read_csv(GENERATED_REPORT_CSV)\n",
    "print(\"len(generated_report_df):\", len(generated_report_df))\n",
    "generated_report_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "g-eval start with **gpt-4**\n",
      "\n",
      "=====================================================\n",
      "[G-EVAL] aspect=consistency, gpt_text=4\n",
      "[G-EVAL] aspect=coherence, gpt_text=5\n",
      "[G-EVAL] aspect=fluency, gpt_text=1\n",
      "[G-EVAL] aspect=relevance, gpt_text=4\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "results[\"g-eval\"] = calculate_g_eval(\n",
    "    source_texts=generated_report_df[\"source\"].tolist(),\n",
    "    generated_texts=generated_report_df[\"report\"].tolist(),\n",
    "    eval_type=\"report\",\n",
    "    model_name=Config.config[\"summary\"][\"g_eval\"][\"openai_model\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== REPORT Evaluation Results =====\n",
      "\n",
      "--- Report Sample 1 ---\n",
      "[G-EVAL] consistency=4.0000, coherence=5.0000, fluency=1.0000, relevance=4.0000\n",
      "\n",
      "===== Averages =====\n",
      "\n",
      "[G-EVAL Avg]\n",
      "  consistency=4.0000, coherence=5.0000, fluency=1.0000, relevance=4.0000\n"
     ]
    }
   ],
   "source": [
    "print_evaluation_results(results, eval_type=\"report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
